
@incollection{goos_differential_2003,
	location = {Berlin, Heidelberg},
	title = {A Differential Fault Attack Technique against {SPN} Structures, with Application to the {AES} and Khazad},
	volume = {2779},
	isbn = {978-3-540-40833-8 978-3-540-45238-6},
	url = {http://link.springer.com/10.1007/978-3-540-45238-6_7},
	abstract = {In this paper we describe a diﬀerential fault attack technique working against Substitution-Permutation Networks, and requiring very few faulty ciphertexts. The fault model used is realistic, as we consider random faults aﬀecting bytes (faults aﬀecting one only bit are much harder to induce). We implemented our attack on a {PC} for both the {AES} and {KHAZAD}. We are able to break the {AES}-128 with only 2 faulty ciphertexts, assuming the fault occurs between the antepenultimate and the penultimate {MixColumn}; this is better than the previous fault attacks against {AES}[6,10,11]. Under similar hypothesis, {KHAZAD} is breakable with 3 faulty ciphertexts.},
	pages = {77--88},
	booktitle = {Cryptographic Hardware and Embedded Systems - {CHES} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Piret, Gilles and Quisquater, Jean-Jacques},
	editor = {Walter, Colin D. and Koç, Çetin K. and Paar, Christof},
	editorb = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2025-12-13},
	date = {2003},
	langid = {english},
	doi = {10.1007/978-3-540-45238-6_7},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/W64EUIEG/Piret and Quisquater - 2003 - A Differential Fault Attack Technique against SPN Structures, with Application to the AES and Khazad.pdf:application/pdf},
}

@inproceedings{lee_benchmarking_2021,
	location = {Virtual {WI} {USA}},
	title = {Benchmarking Video Object Detection Systems on Embedded Devices under Resource Contention},
	isbn = {978-1-4503-8597-8},
	url = {https://dl.acm.org/doi/10.1145/3469116.3470010},
	doi = {10.1145/3469116.3470010},
	abstract = {Adaptive and efficient computer vision systems have been proposed to make computer vision tasks, e.g., object classification and object detection, optimized for embedded boards or mobile devices. These studies focus on optimizing the model (deep network) or system itself, by designing an efficient network architecture or adapting the network architecture at runtime using approximation knobs, such as image size, type of object tracker, head of the object detector (e.g., lighter-weight heads such as one-shot object detectors like {YOLO} over two-shot object detectors like {FRCNN}). In this work, we benchmark different video object detection protocols, including {FastAdapt}, with respect to accuracy, latency, and energy consumption on three different embedded boards that represent the leading edge mobile {GPUs}. Our set of protocols consists of Faster R-{CNN}, {YOLOv}3, {SELSA}, {MEGA}, and {REPP}. Further, we characterize their performance under different levels of resource contention, specifically {GPU} contention, as would arise due to co-located applications on these boards, contending with the video object detection task. Our key insights are that object detectors have to be coupled with trackers to keep up with the latency requirements (e.g., 30 fps). With this, {FastAdapt} achieves up to 76 fps on the most well-resourced {NVIDIA} Jetson-class board—the {NVIDIA} {AGX} Xavier. Second, adaptive protocols like {FastAdapt}, {FRCNN}, and {YOLO} (specifically our adaptive variants, {FRCNN}+ and {YOLO}+) work well under resource constraints. Among the latest video object detection heads, {SELSA} achieves the highest accuracy but at a latency of over 2 sec per frame. Our energy consumption experiments bring out that {FastAdapt}, adaptive {FRCNN}, and adaptive {YOLO} are best-in-class, relative to the non-adaptive protocols {SELSA}, {MEGA}, and {REPP}.},
	eventtitle = {{MobiSys} '21: The 19th Annual International Conference on Mobile Systems, Applications, and Services},
	pages = {19--24},
	booktitle = {Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning},
	publisher = {{ACM}},
	author = {Lee, Jayoung and Wang, Pengcheng and Xu, Ran and Dasari, Venkat and Weston, Noah and Li, Yin and Bagchi, Saurabh and Chaterji, Somali},
	urldate = {2025-11-07},
	date = {2021-06-25},
	langid = {english},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/CHYX5VY9/Lee et al. - 2021 - Benchmarking Video Object Detection Systems on Embedded Devices under Resource Contention.pdf:application/pdf},
}

@article{cantero_benchmarking_2022,
	title = {Benchmarking Object Detection Deep Learning Models in Embedded Devices},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/11/4205},
	doi = {10.3390/s22114205},
	abstract = {Object detection is an essential capability for performing complex tasks in robotic applications. Today, deep learning ({DL}) approaches are the basis of state-of-the-art solutions in computer vision, where they provide very high accuracy albeit with high computational costs. Due to the physical limitations of robotic platforms, embedded devices are not as powerful as desktop computers, and adjustments have to be made to deep learning models before transferring them to robotic applications. This work benchmarks deep learning object detection models in embedded devices. Furthermore, some hardware selection guidelines are included, together with a description of the most relevant features of the two boards selected for this benchmark. Embedded electronic devices integrate a powerful {AI} co-processor to accelerate {DL} applications. To take advantage of these co-processors, models must be converted to a speciﬁc embedded runtime format. Five quantization levels applied to a collection of {DL} models are considered; two of them allow the execution of models in the embedded general-purpose {CPU} and are used as the baseline to assess the improvements obtained when running the same models with the three remaining quantization levels in the {AI} co-processors. The benchmark procedure is explained in detail, and a comprehensive analysis of the collected data is presented. Finally, the feasibility and challenges of the implementation of embedded object detection applications are discussed.},
	pages = {4205},
	number = {11},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Cantero, David and Esnaola-Gonzalez, Iker and Miguel-Alonso, Jose and Jauregi, Ekaitz},
	urldate = {2025-11-07},
	date = {2022-05-31},
	langid = {english},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/YUETKGK6/Cantero et al. - 2022 - Benchmarking Object Detection Deep Learning Models in Embedded Devices.pdf:application/pdf},
}

@article{qasaimeh_benchmarking_2021,
	title = {Benchmarking vision kernels and neural network inference accelerators on embedded platforms},
	volume = {113},
	issn = {13837621},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1383762120301697},
	doi = {10.1016/j.sysarc.2020.101896},
	abstract = {Developing efficient embedded vision applications requires exploring various algorithmic optimization tradeoffs and a broad spectrum of hardware architecture choices. This makes navigating the solution space and finding the design points with optimal performance trade-offs a challenge for developers. To help provide a fair baseline comparison, we conducted comprehensive benchmarks of accuracy, run-time, and energy efficiency of a wide range of vision kernels and neural networks on multiple embedded platforms: {ARM}57 {CPU}, Nvidia Jetson {TX}2 {GPU} and Xilinx {ZCU}102 {FPGA}. Each platform utilizes their optimized libraries for vision kernels ({OpenCV}, {VisionWorks} and {xfOpenCV}) and neural networks ({OpenCV} {DNN}, {TensorRT} and Xilinx {DPU}). For vision kernels, our results show that the {GPU} achieves an energy/frame reduction ratio of 1.1–3.2× compared to the others for simple kernels. However, for more complicated kernels and complete vision pipelines, the {FPGA} outperforms the others with energy/frame reduction ratios of 1.2–22.3×. For neural networks [Inception-v2 and {ResNet}-50, {ResNet}-18, Mobilenet-v2 and {SqueezeNet}], it shows that the {FPGA} achieves a speed up of [2.5, 2.1, 2.6, 2.9 and 2.5]× and an {EDP} reduction ratio of [1.5, 1.1, 1.4, 2.4 and 1.7]× compared to the {GPU} {FP}16 implementations, respectively.},
	pages = {101896},
	journaltitle = {Journal of Systems Architecture},
	shortjournal = {Journal of Systems Architecture},
	author = {Qasaimeh, Murad and Denolf, Kristof and Khodamoradi, Alireza and Blott, Michaela and Lo, Jack and Halder, Lisa and Vissers, Kees and Zambreno, Joseph and Jones, Phillip H.},
	urldate = {2025-11-07},
	date = {2021-02},
	langid = {english},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/FG3LTB3G/Qasaimeh et al. - 2021 - Benchmarking vision kernels and neural network inference accelerators on embedded platforms.pdf:application/pdf},
}

@inproceedings{lazarevich_yolobench_2023,
	location = {Paris, France},
	title = {{YOLOBench}: Benchmarking Efficient Object Detectors on Embedded Systems},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0744-3},
	url = {https://ieeexplore.ieee.org/document/10350386/},
	doi = {10.1109/ICCVW60793.2023.00126},
	shorttitle = {{YOLOBench}},
	abstract = {We present {YOLOBench}, a benchmark comprised of 550+ {YOLO}-based object detection models on 4 different datasets and 4 different embedded hardware platforms (x86 {CPU}, {ARM} {CPU}, Nvidia {GPU}, {NPU}). We collect accuracy and latency numbers for a variety of {YOLO}-based one-stage detectors at different model scales by performing a fair, controlled comparison of these detectors with a ﬁxed training environment (code and training hyperparameters). Pareto-optimality analysis of the collected data reveals that, if modern detection heads and training techniques are incorporated into the learning process, multiple architectures of the {YOLO} series achieve a good accuracylatency trade-off, including older models like {YOLOv}3 and {YOLOv}4. We also evaluate training-free accuracy estimators used in neural architecture search on {YOLOBench} and demonstrate that, while most state-of-the-art zero-cost accuracy estimators are outperformed by a simple baseline like {MAC} count, some of them can be effectively used to predict Pareto-optimal detection models. We showcase that by using a zero-cost proxy to identify a {YOLO} architecture competitive against a state-of-the-art {YOLOv}8 model on a Raspberry Pi 4 {CPU}. The code and data are available at https://github.com/Deeplite/deeplitetorch-zoo.},
	eventtitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision Workshops ({ICCVW})},
	pages = {1161--1170},
	booktitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision Workshops ({ICCVW})},
	publisher = {{IEEE}},
	author = {Lazarevich, Ivan and Grimaldi, Matteo and Kumar, Ravish and Mitra, Saptarshi and Khan, Shahrukh and Sah, Sudhakar},
	urldate = {2025-11-07},
	date = {2023-10-02},
	langid = {english},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/FNUNLWDK/Lazarevich et al. - 2023 - YOLOBench Benchmarking Efficient Object Detectors on Embedded Systems.pdf:application/pdf},
}

@article{berthelier_deep_2021,
	title = {Deep Model Compression and Architecture Optimization for Embedded Systems: A Survey},
	volume = {93},
	issn = {1939-8018, 1939-8115},
	url = {https://link.springer.com/10.1007/s11265-020-01596-1},
	doi = {10.1007/s11265-020-01596-1},
	shorttitle = {Deep Model Compression and Architecture Optimization for Embedded Systems},
	abstract = {Over the past, deep neural networks have proved to be an essential element for developing intelligent solutions. They have achieved remarkable performances at a cost of deeper layers and millions of parameters. Therefore utilising these networks on limited resource platforms for smart cameras is a challenging task. In this context, models need to be (i) accelerated and (ii) memory efficient without significantly compromising on performance. Numerous works have been done to obtain smaller, faster and accurate models. This paper presents a survey of methods suitable for porting deep neural networks on resource-limited devices, especially for smart cameras. These methods can be roughly divided in two main sections. In the first part, we present compression techniques. These techniques are categorized into: knowledge distillation, pruning, quantization, hashing, reduction of numerical precision and binarization. In the second part, we focus on architecture optimization. We introduce the methods to enhance networks structures as well as neural architecture search techniques. In each of their parts, we describe different methods, and analyse them. Finally, we conclude this paper with a discussion on these methods.},
	pages = {863--878},
	number = {8},
	journaltitle = {Journal of Signal Processing Systems},
	shortjournal = {J Sign Process Syst},
	author = {Berthelier, Anthony and Chateau, Thierry and Duffner, Stefan and Garcia, Christophe and Blanc, Christophe},
	urldate = {2025-11-06},
	date = {2021-08},
	langid = {english},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/V8EJKKF7/Berthelier et al. - 2021 - Deep Model Compression and Architecture Optimization for Embedded Systems A Survey.pdf:application/pdf},
}

@article{chakraborty_introduction_2022,
	title = {Introduction to the Special Issue on Embedded Systems for Computer Vision},
	volume = {8},
	rights = {Creative Commons Attribution 4.0 International license, info:eu-repo/semantics/{openAccess}},
	issn = {2199-2002},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/LITES.8.1.0},
	doi = {10.4230/LITES.8.1.0},
	abstract = {We provide a broad overview of some of the current research directions at the intersection of embedded systems and computer vision, in addition to introducing the papers appearing in this special issue. Work at this intersection is steadily growing in importance, especially in the context of autonomous and cyber-physical systems design. Vision-based perception is almost a mandatory component in any autonomous system, but also adds myriad challenges like, how to efficiently implement vision processing algorithms on resource-constrained embedded architectures, and how to verify the functional and timing correctness of these algorithms. Computer vision is also crucial in implementing various smart functionality like security, e.g., using facial recognition, or monitoring events or traffic patterns. Some of these applications are reviewed in this introductory article. The remaining articles featured in this special issue dive into more depth on a few of them.},
	pages = {0:i--0:viii},
	number = {1},
	journaltitle = {Leibniz Transactions on Embedded Systems ({LITES})},
	author = {Chakraborty, Samarjit and Rao, Qing},
	urldate = {2025-11-06},
	date = {2022},
	langid = {english},
	note = {Artwork Size: 8 pages, 489604 bytes
Medium: application/pdf
Publisher: Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	keywords = {Computer architecture, Computer systems organization → Embedded and cyber-physical systems, Computer vision, Cyber-physical systems, Embedded systems},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/2SR2B3FY/Chakraborty and Rao - 2022 - Introduction to the Special Issue on Embedded Systems for Computer Vision.pdf:application/pdf},
}

@article{silva_method_2020,
	title = {A method for embedding a computer vision application into a wearable device},
	volume = {76},
	issn = {01419331},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933119304089},
	doi = {10.1016/j.micpro.2020.103086},
	abstract = {Pattern classiﬁcation applications can be found everywhere, especially the ones that use computer vision. What makes them diﬃcult to embed is the fact that they often require a lot of computational resources. Embedded computer vision has been applied in many contexts, such as industrial or home automation, robotics, and assistive technologies. This work performs a design space exploration in an image classiﬁcation system and embeds a computer vision application into a minimum resource platform, targeting wearable devices. The feature extractor and the classiﬁer are evaluated for memory usage and computation time. A method is proposed to optimize such characteristics, leading to a reduction of over 99\% in computation time and 92\% in memory usage, with respect to a standard implementation. Experimental results in an {ARM} Cortex-M platform showed a total classiﬁcation time of 0.3 s, maintaining the same accuracy as in the simulation performed. Furthermore, less than 20 {KB} of data memory was required, which is the most limited resource available in low-cost and low-power microcontrollers. The target application, used for the experimental evaluation, is a crosswalk detector used to help visually impaired persons.},
	pages = {103086},
	journaltitle = {Microprocessors and Microsystems},
	shortjournal = {Microprocessors and Microsystems},
	author = {Silva, Elias T. and Sampaio, Fausto and Da Silva, Lucas C. and Medeiros, David S. and Correia, Gustavo P.},
	urldate = {2025-11-06},
	date = {2020-07},
	langid = {english},
	file = {PDF:/Users/selimbenbouzid/Zotero/storage/VSZ6JNEQ/Silva et al. - 2020 - A method for embedding a computer vision application into a wearable device.pdf:application/pdf},
}
