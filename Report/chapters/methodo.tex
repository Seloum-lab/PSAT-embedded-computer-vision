\section{Méthodologie}
{Afin d'effectuer au mieux le benchmark, il faut tout d'abord définir précisément la méthodologie utilisée. Nous allons 
d'abord définir les différentes métriques que nous allons comparer et détailler la méthode de mesure associée, 
nous allons ensuite lister les différents modèles que nous avons décidés de comparer entre eux pour enfin spécifier quel aura 
été le support, c'est à dire le hardware.}

\subsection{Métriques}
{Dans le contexte de la vision par ordinateur, il y a un nombre important de métriques à prendre en compte; des métriques qui 
représentent par exemple l'efficacité et/ou la précision. Cependant, en ajoutant la dimension des systèmes embarqués, il y a alors 
encore plus de métriques qui deviennent intéressantes. De plus, certaines métriques deviennent inintéressantes isolées et doivent 
nécessairement être couplées.}

\subsubsection{Efficacité}
{La première métrique sur laquelle nous allons nous pencher est celle de l'efficacité, celle des FLOPS. Cette métrique étant particulièrement 
compliqué à mesurer (car le temps d'exécution dépend autant de la mémoire, du parallélisme et du runtime que du nombre d'opérations 
arithmétiques réellement effectuées), nous allons donc nous baser sur les FLOPS théorique du modèle. Cette grandeur représente le coût 
de calcul d'une inférence du point de vue purement algorithmique. Cela calcule théoriquement le nombre d'opérations arithmétiques nécessaires 
étant donné un modèle et la taille de l'entrée. Bien que cette mesure soit importante du point de vue du modèle pour estimer son efficacité, elle 
est totalement décorrélée du hardware et est donc à prendre avec des pincettes pour le choix du couple machine-modèle.}

\subsubsection{Performance}
{Ensuite, nous nous intéresserons aux métriques de performance. Nous allons nous pencher tout d'abord sur la latence, c'est à dire la capacité du 
couple modèle-machine à traiter les image le plus vite possible. Cette métrique représente le temps nécessaire au couple pour traiter une image et 
cela se mesure en unité temporelle. Ensuite, nous nous intéresserons aussi au débit. Cela représente la quantité d'information traitée par le couple par unité 
de temps. Dans ce cadre, la mesure est faite en FPS (Frame Per Second) et représente le nombre d'images traitées par seconde. Bien qu'il y ait une corrélation forte 
entre ces deux grandeurs, il est quand même important de les prendre toutes deux en compte. On peut en effet avoir des modèles qui traitent l'image quasiment 
instantanément, mais en un temps important, tout comme des modèles qui au contraire ont besoin de temps pour traiter l'image, mais peuvent en traiter plusieurs à 
la fois. Prioriser l'un ou l'autre ou les deux dépendra énormément du cadre d'application et des nécessités de celui-ci.}

\subsubsection{Précision}
{Maintenant que nous nous sommes intéressés à l'efficacité et à la performance, il est temps de s'intéresser à la précision. Cette précision est elle quasiment uniquement 
déterminée par le modèle, mais un hardware très limité peut avoir un impact (encodage des floats, pruning propre du hardware, etc).
Cette précision, nous allons la déterminer grâce à deux métriques; la précision et le rappel.

Pour ces deux métriques, on définit d'abord :

\begin{itemize}
    \item $TP$ (True Positives) : nombre de prédictions correctes pour la classe,
    \item $FP$ (False Positives) : nombre de prédictions incorrectes pour la classe,
    \item $FN$ (False Negatives) : nombre de vrais objets non détectés par le modèle.
\end{itemize}

La précision mesure la proportion de prédictions correctes parmi toutes celles effectuées par le modèle :
\[
\text{Précision} = \frac{TP}{TP + FP}
\]
Elle indique la fiabilité du modèle lorsqu'il prédit un objet.

Le rappel mesure la proportion de vrais objets correctement détectés parmi tous les objets réels :
\[
\text{Rappel} = \frac{TP}{TP + FN}
\]
Il reflète la capacité du modèle à ne rien manquer.

On peut également combiner ces deux aspects avec le \textit{F1-score} :
\[
F_1 = 2 \cdot \frac{\text{Précision} \cdot \text{Rappel}}{\text{Précision} + \text{Rappel}}
\]
utile pour évaluer le compromis entre fiabilité des prédictions et couverture des objets réels.
La manière de définir la véracité de la prédiction (TP, FP ou FN) peut grandement varier en fonction du domaine applicatif. Dans celui de la computer vision, 
il y a des protocoles déjà bien définis basés sur l'IoU (Intersecion over Union). Nous détaillerons dans la partie suivante ce protocole.}


\subsubsection{Puissance}
{Enfin, la dernière métrique sur laquelle nous allons nous pencher est celle de la puissance. Cette métrique nous intéresse particulièrement du point de vue 
des systèmes embarqués car elle permet d’évaluer non seulement l’efficacité énergétique de l’algorithme, mais aussi son impact sur l’autonomie et la chaleur 
générée par le matériel. Dans les systèmes embarqués, où les ressources sont limitées et où la dissipation thermique doit être maîtrisée, optimiser la puissance 
consommée devient aussi crucial que maximiser la précision ou l'efficacité.

}

\subsection{Protocole de mesure}
{Maintenant que nous avons défini les métriques que nous prenons en compte, nous allons expliquer rigoureusement nos méthodes de mesure.}

\subsubsection{Efficacité}
{Comme dit précédemment, les FLOPS mesurés sont des FLOPS théoriques qui ne dépendent que du modèle et de l'input. en l'occurence, la taille de l'input est en fait la 
taille de l'image qui est prédéfinie pour chaque modèle (nous préciserons cette taille dans une partie future).
Pour effectuer ce calcul théoriques, nous nous sommes appuyés sur des librairies pré-existantes, chacune calculant le résultat théorique pour un type de modèle.
Pour les modèles supporté sur $PyTorch$, nous avons utilisé l'API $thop$, pour les modèles supportés sur $ONXX$\footnote{Il a été impossible de faire cela pour le modèle MobileNet SSD supporté par ONXX}, nous avons utilisé l'API $ONXX-tools$, 
et enfin pour les modèles $YOLO$, nous nous sommes basés sur $Ultralytics$.}

\subsubsection{Performance}
Afin de mesurer les performances, nous nous sommes intéressés à la latence pendant l'inférence, ainsi qu'au debit en FPS.
Pour ce faire, nous avons intégré une mesure du temps d'inférence directement dans le code de test.
Les mesures ont été stockées dans un fichier pour chaque exécution de 30 secondes, puis nous avons calculé la moyenne pour 10 exécutions consécutives.

\subsubsection{Précision}

\subsubsection{Puissance}
Dans le but de mesurer la puissance consommée par le système lors de l'exécution des modèles, nous avons utilisé un wattmètre branché sur la prise secteur alimentant la Raspberry Pi.
Ce dispositif nous a permis de capturer la consommation électrique en temps réel pendant et en dehors des périodes d'inférence.
Cependant le dispositif étant plutôt adapté pour des mesures plus importantes, les mesures ont une certaine incertitude.
De plus, nous n'avions accès qu'à la valeur instantanée, sans moyen d'enregistrer les données afin de les agréger.
Ainsi, nous avons relevé manuellement la consommation avant le début de l'inférence et pendant, en effectuant une moyenne si les valeurs variaient.


\subsection{Les modèles}
Pour effectuer ce benchmark, nous avons utilisé différents modèles afin de les comparer, mais aussi différents frameworks de support pour ces derniers. Voici 
la liste exhaustive des modèles comparés, et pour chacun d'entre eux le framework de support.

\subsubsection{YOLOv5}
YOLOv5 (You Only Look Once) est une famille de modèles de détection d'objets en temps réel développée par Ultralytics.
Sorti en 2020, YOLOv5 est conçu pour être rapide et efficace, permettant des performances élevées même sur des matériels limités.
Il utilise une architecture de réseau de neurones convolutifs (CNN) optimisée pour la détection d'objets, avec des améliorations telles que des couches de convolution plus profondes et des techniques de régularisation avancées.
YOLOv5 est disponible en plusieurs variantes (nano, small, medium, large, xlarge) pour s'adapter à différents besoins en termes de vitesse et de précision.
Dans le cadre de ce benchmark, nous avons utilisé la version YOLOv5n (nano) pour maximiser la vitesse d'inférence sur les systèmes embarqués.

\subsubsection{YOLOv11}
YOLOv11 est une des dernières itérations de la série YOLO, sorti en 2024, introduisant des améliorations significatives en termes de précision et d'efficacité.
Il intègre des techniques avancées telles que l'attention spatiale et des modules de fusion de caractéristiques pour améliorer la détection d'objets dans des environnements complexes.
YOLOv11 est conçu pour être encore plus rapide que ses prédécesseurs, tout en maintenant une haute précision, ce qui le rend idéal pour les applications en temps réel sur des dispositifs embarqués.
Pour ce benchmark, nous avons utilisé la version YOLOv11n (nano), qui est optimisée pour les performances sur des matériels à ressources limitées.
Cependant, plus de précision implique souvent plus de calcul, et donc un compromis à évaluer.

\subsubsection{FasterCNN}
Sorti en 2015, Faster R-CNN (Region-based Convolutional Neural Networks) est une architecture de détection d'objets qui a révolutionné le domaine en introduisant un réseau de propositions de régions (RPN) pour générer des propositions d'objets de manière efficace.
Faster R-CNN utilise un réseau de neurones convolutifs pour extraire des caractéristiques de l'image, suivi par le RPN qui propose des régions d'intérêt susceptibles de contenir des objets.
Ces régions sont ensuite affinées et classifiées par un réseau de classification.
Faster R-CNN est connu pour sa haute précision, mais il est généralement plus lent que des modèles plus récents basés sur des architectures différentes.
Dans ce benchmark, nous avons utilisé une version allégée de Faster R-CNN basée sur un backbone MobileNetV3 pour améliorer la vitesse d'inférence sur les systèmes embarqués.

\subsubsection{MobileNet SSD}
MobileNet SSD (Single Shot MultiBox Detector) est une architecture de détection d'objets légère et efficace sortie en 2017.
Elle combine le réseau MobileNet, conçu pour être rapide et peu gourmand en ressources, avec le SSD, qui permet de détecter des objets en une seule passe à travers l'image.
MobileNet SSD est particulièrement adapté aux applications mobiles et embarquées en raison de sa faible empreinte mémoire et de sa rapidité d'inférence.
Il utilise des convolutions séparables en profondeur pour réduire le nombre de paramètres tout en maintenant une bonne précision.


\subsection{Le hardware}
{Pour effectuer le banchmark, nous avons utilisé deux supports différents de systèmes embarqués, tous deux des Rasberry Pi.}

\subsubsection{Rasberry Pi 4}
{La Raspberry Pi 4 est équipée d’un processeur Quad-Core Cortex-A72 cadencé à 1,5 GHz et dispose de 4 Go de RAM. 
Pour la capture d’images, nous utilisons le module officiel Raspberry Pi Camera Module 3, qui se connecte via l’interface CSI et permet des 
prises de vue haute résolution et à grande vitesse. Cette caméra n’est compatible qu’avec la Pi 4, mais nous utilisons également une caméra Logitech 
C860 USB, qui fonctionne avec le Pi 4 et fournit une solution alternative pour la capture d’images standard.}

\subsubsection{Rasberry Pi 5}
{La Raspberry Pi 5 apporte des améliorations notables en performance, avec un processeur plus rapide et une meilleure gestion de l’USB 
et de l’affichage. En revanche, la Raspberry Pi Camera Module 3 n’est pas compatible avec ce modèle. Pour la capture d'images nous utilisons exclusivement 
la caméra Logitech USB, qui assure une compatibilité totale et un flux vidéo stable pour les deux générations de Raspberry Pi.}
